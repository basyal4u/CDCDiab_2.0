{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2em;color:purple; font-style:bold\"><br>\n",
    "<b>Deep Neural Networks to Predict <br><br>Self-perception of Cardiovascular Disease\n",
    "<br><br>~ A Technical Demo</b><br></p>\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"><br>David Zeng Ph.D.</p>\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\">Assistant Professor/Researcher, Masters of Science in Analytics Program</p>\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\">College of Business and Information Systems, Dakota State University</p>\n",
    "\n",
    "<br><br>\n",
    "### The objective of this research is three-fold:\n",
    "#### * Better understanding of how well DNN would improve the accuracy of prediction on perception of cardiovascular disease\n",
    "#### * Framework of developing more sophisticated DNN models to predict medical outcomes\n",
    "#### * Foundation for learning multi-dimensional/distributed representation of healthcare concepts that are both intepretable and scalable\n",
    "\n",
    "### I train a multiple-hidden-layer deep neural network with a large data set of 1729 features and about 30,000 samples. The dataset is CDC  Demographics, Dietary Data, Examination, Laboratory, and Questionnaire datasets collected from 1999 to 2016. Substantial data cleaning and pre-processing are done with Python pandas library. <p>\n",
    "    \n",
    "#### Technical highlights:\n",
    "* Open-source libraries/models\n",
    "* Python Library for Deep Learning keras\n",
    "* TensorFlow as the backend\n",
    "* Multiclass classification with softmax output layer and cross-entropy loss\n",
    "* 1 128-dimensional hidden layer with relu activations\n",
    "* 2 64-dimensional hidden layers with relu activations\n",
    "* 0.5 drop-out following each hidden layer\n",
    "* Commonly-accepted optimizer: SGD with Nesterov Momentum\n",
    "* L2 weight regularization\n",
    "* 1729 input features\n",
    "* 10-class target: from patient questionnaire data\n",
    "* Data transformations: 1729 - 128 - 64 - 64 - 10\n",
    "* Mini batch-size = 512\n",
    "* Optimal number of epochs = 100\n",
    "* Input features are standardized\n",
    "* Missing values are filled with means\n",
    "* Base models (LogReg, kNN, DT, Shallow NNET, and SVM) are implemented with scikit-learn\n",
    "* Models are trained on both CPU and GPU\n",
    "* GPU environment: two GTX 1080 Ti 8GB cards\n",
    "* A subset of the data with about 3,200 examples is used for this demonstration\n",
    "* About 600 iterations to complete 100 epoch\n",
    "* About 1 million parameters in the DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Data Files into the training dataset\n",
    "#### 1. Load all files into a list\n",
    "#### 2. Merge them into a big pandas DataFrame with a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = \"C:\\\\Users\\\\dzeng\\\\DL Research\\\\Data\\\\1999\\\\\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.xpt\"))\n",
    "\n",
    "\n",
    "data = np.array([np.arange(1,10001)]).T\n",
    "x_train =pd.DataFrame(data, index = range(0,10000),columns=['SEQN'])\n",
    "\n",
    "for infile in allFiles:\n",
    "    df = pd.read_sas(infile)\n",
    "    df['SEQN']=df['SEQN'].astype(int)\n",
    "    x_train = x_train.merge(df.drop_duplicates(subset=['SEQN']), on='SEQN', how='left')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1729)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing empty rows from DataFrame in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_noemptyrows = x_train.dropna(axis=0, thresh=20, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9965, 1729)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_noemptyrows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_noemptyrows.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = x_train_noemptyrows.fillna(x_train_noemptyrows.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "# test_data -= mean\n",
    "# test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding target into the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.read_sas(\"C:\\\\Users\\\\dzeng\\\\DL Research\\\\Data\\\\target\\\\CDQ.xpt\")\n",
    "#full_dataset = full_data.merge(y_train,on='SEQN', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_clean = y_train.drop(['CDQ030', 'CDQ040','CDQ060','CDQ090'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3185, 6)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Meraging into a full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_dataset = pd.merge(y_train_clean, train_data, on='SEQN', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the dataset into training (75%) and test (25%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "full_dataset[:,0:1728], full_dataset[:,1728:], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the target\n",
    "One-hot encoding is a widely used format for categorical data, also called categorical encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def to_one_hot(labels, dimension=10):\n",
    "results = np.zeros((len(labels), dimension))\n",
    "for i, label in enumerate(labels):\n",
    "results[i, label] = 1.\n",
    "return results\n",
    "\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: Deep Neural Networks for multi-class softmax classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 200))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 200))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "\n",
    "model.add(Dense(128, kernel_regularizer=regularizers.l2(0.001),\n",
    "    activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, kernel_regularizer=regularizers.l2(0.001),\n",
    "    activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, kernel_regularizer=regularizers.l2(0.001),\n",
    "    activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dropout\n",
    "Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Geoff Hinton and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training. Let’s say a given layer would normally return a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training. After applying dropout, this vector will have a few zero entries distributed at random: for example, [0, 0.5, 1.3, 0, 1.1]. The dropout rate is the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5. At test time, no units are dropped out; instead, the layer’s output values are scaled down by a factor equal to the dropout rate, to balance for the fact that more units are active than at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting aside a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:500]\n",
    "partial_x_train = x_train[500:]\n",
    "y_val = one_hot_train_labels[:500]\n",
    "partial_y_train = one_hot_train_labels[500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,partial_y_train,\n",
    "    epochs=100,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining a model from scratch with the optimal number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train, partial_y_train, epochs=9,\n",
    "    batch_size=512, validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels,batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the final results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This approach reaches an accuracy of  about 80% while the best accuracy reached by a base-model classifier is 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
